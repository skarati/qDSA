#ifndef MULT_H_
#define MULT_H_

#define ADD(C,A,B) {C = A+B;}
#define SUB(C,A,B) {C = A-B;}
#define MULT(C,A,B) {C = A*B;}
#define SHIFTL(C,A,B) {C = A << (B);}
#define SHIFTR(C,A,B) {C = A >> (B);}
#define AND(C,A,B) {C = A & B;}

#define ADD128U(C,A,B) {C = A+B;}
#define SUB128U(C,A,B) {C = A-B;}
#define MULT128U(C,A,B) {C = (u128)(A) * B;} // multiplies two 64-bit values and returns a 128-bit value
#define SHIFTL128U(C,A,B) {C = A << (B);}
#define SHIFTR128U(C,A,B) {C = A >> (B);}
#define AND128U(C,A,B) {C = A & B;}




u128 MULT128U10S1, MULT128U10S2,MULT10128US1;
u64 MULT9S1;

#define REDUCE2519(RES0,RES1,RES2,RES3,RES4,RES5,RES6,RES7,RES8,TEMP) {\
\
	MULT(TEMP[9],TEMP[9],18ULL); MULT(TEMP[10],TEMP[10],18ULL); MULT(TEMP[11],TEMP[11],18ULL); MULT(TEMP[12],TEMP[12],18ULL); \
	MULT(TEMP[13],TEMP[13],18ULL); MULT(TEMP[14],TEMP[14],18ULL); MULT(TEMP[15],TEMP[15],18ULL); MULT(TEMP[16],TEMP[16],18ULL); \
 \
	ADD(RES0,TEMP[0],TEMP[9]); ADD(RES1,TEMP[1],TEMP[10]); ADD(RES2,TEMP[2],TEMP[11]); ADD(RES3,TEMP[3],TEMP[12]); \
	ADD(RES4,TEMP[4],TEMP[13]); ADD(RES5,TEMP[5],TEMP[14]); ADD(RES6,TEMP[6],TEMP[15]); ADD(RES7,TEMP[7],TEMP[16]); RES8=TEMP[8];\
 \
	SHIFTR(MULT9S1,RES0,28); AND(RES0,RES0,mask28); ADD(RES1,RES1,MULT9S1); \
	SHIFTR(MULT9S1,RES1,28); AND(RES1,RES1,mask28); ADD(RES2,RES2,MULT9S1); \
	SHIFTR(MULT9S1,RES2,28); AND(RES2,RES2,mask28); ADD(RES3,RES3,MULT9S1); \
	SHIFTR(MULT9S1,RES3,28); AND(RES3,RES3,mask28); ADD(RES4,RES4,MULT9S1); \
	SHIFTR(MULT9S1,RES4,28); AND(RES4,RES4,mask28); ADD(RES5,RES5,MULT9S1); \
	SHIFTR(MULT9S1,RES5,28); AND(RES5,RES5,mask28); ADD(RES6,RES6,MULT9S1); \
	SHIFTR(MULT9S1,RES6,28); AND(RES6,RES6,mask28); ADD(RES7,RES7,MULT9S1); \
	SHIFTR(MULT9S1,RES7,28); AND(RES7,RES7,mask28); ADD(RES8,RES8,MULT9S1); \
	SHIFTR(MULT9S1,RES8,27); AND(RES8,RES8,mask27); \
	MULT(MULT9S1,MULT9S1,9ULL); ADD(RES0,RES0,MULT9S1); \
	SHIFTR(MULT9S1,RES0,28); AND(RES0,RES0,mask28); ADD(RES1,RES1,MULT9S1); \
}

#define REDUCEPARTB2519(RES0,RES1,RES2,RES3,RES4,RES5,RES6,RES7,RES8) { \
\
	SHIFTR(MULT9S1,RES0,28); AND(RES0,RES0,mask28); ADD(RES1,RES1,MULT9S1); \
	SHIFTR(MULT9S1,RES1,28); AND(RES1,RES1,mask28); ADD(RES2,RES2,MULT9S1); \
	SHIFTR(MULT9S1,RES2,28); AND(RES2,RES2,mask28); ADD(RES3,RES3,MULT9S1); \
	SHIFTR(MULT9S1,RES3,28); AND(RES3,RES3,mask28); ADD(RES4,RES4,MULT9S1); \
	SHIFTR(MULT9S1,RES4,28); AND(RES4,RES4,mask28); ADD(RES5,RES5,MULT9S1); \
	SHIFTR(MULT9S1,RES5,28); AND(RES5,RES5,mask28); ADD(RES6,RES6,MULT9S1); \
	SHIFTR(MULT9S1,RES6,28); AND(RES6,RES6,mask28); ADD(RES7,RES7,MULT9S1); \
	SHIFTR(MULT9S1,RES7,28); AND(RES7,RES7,mask28); ADD(RES8,RES8,MULT9S1); \
	SHIFTR(MULT9S1,RES8,27); AND(RES8,RES8,mask27); \
	MULT(MULT9S1,MULT9S1,9ULL); ADD(RES0,RES0,MULT9S1); \
	SHIFTR(MULT9S1,RES0,28); AND(RES0,RES0,mask28); ADD(RES1,RES1,MULT9S1); \
}

#define REDUCEPARTB2519_51(RES0,RES1,RES2,RES3,RES4) { \
\
	SHIFTR128U(MULT128U10S1,RES0,54); AND128U(RES0,RES0,mask51); ADD128U(RES1,RES1,MULT128U10S1); \
	SHIFTR128U(MULT128U10S1,RES1,54); AND128U(RES1,RES1,mask51); ADD128U(RES2,RES2,MULT128U10S1); \
	SHIFTR128U(MULT128U10S1,RES2,54); AND128U(RES2,RES2,mask51); ADD128U(RES3,RES3,MULT128U10S1); \
	SHIFTR128U(MULT128U10S1,RES3,54); AND128U(RES3,RES3,mask51); ADD128U(RES4,RES4,MULT128U10S1); \
	SHIFTR128U(MULT128U10S1,RES4,50); AND128U(RES4,RES4,mask47); \
\
	SHIFTL128U(MULT128U10S2,MULT128U10S1,3); ADD128U(MULT128U10S2,MULT128U10S1,MULT128U10S2); \
	ADD128U(RES0,RES0,MULT128U10S2); \
	SHIFTR128U(MULT10128US1,RES0,54); AND128U(RES0,RES0,mask51); ADD128U(RES1,RES1,MULT10128US1); \
}

#endif
